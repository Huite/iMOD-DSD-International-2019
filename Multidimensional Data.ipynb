{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multidimensional data\n",
    "=====================\n",
    "\n",
    "The **Basics** tutorial has dealt with reading data, doing arithmetic,\n",
    "intermediate inspection, and finally writing the results back to disk.\n",
    "\n",
    "The example focused on familiar two dimensional data. However, xarray's power\n",
    "becomes more apparent  when dealing with problems that have more than two\n",
    "dimensions.\n",
    "\n",
    "Once again, we've prepared some example data. This time around, it's a\n",
    "geological layer model that one of your geological friends might have cooked\n",
    "up, as well as some simulation results.\n",
    "\n",
    "The ``imod`` package provides functions to easily open these data, even\n",
    "if they are stored in separate two-dimensional files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imod\n",
    "# Shut up some annoying warnings as well\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = imod.idf.open(\"data/layers/top_l*.idf\")\n",
    "bot = imod.idf.open(\"data/layers/bot_l*.idf\")\n",
    "\n",
    "print(top)\n",
    "print(\"\\n\")\n",
    "print(bot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're dealing with three-dimensional data now:\n",
    "\n",
    "1. layer\n",
    "2. y\n",
    "3. x\n",
    "        \n",
    "The ``imod.idf.open()`` function automatically parses the file names to infer\n",
    "layer numbers and timestamps, if applicable. It does so by assuming the files have\n",
    "been named optionally with datetime and layer number (prepended by an \"L\"),\n",
    "separated by underscores, where time and layer are both optional.\n",
    "\n",
    "Of course, there are many cases where file names do not agree with these naming\n",
    "rules. ``imod.idf.open()`` provides some options in this case; [check the\n",
    "documentation](https://imod.xyz/api/idf.html#imod.idf.open).\n",
    "\n",
    "Anyhow, let's compute layer thickness, and check the thickness of the first\n",
    "layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thickness = top - bot\n",
    "thickness.sel(layer=1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.sel()`` selects values along a coordinate (in this case layer), based on value.\n",
    "``.isel()`` selects values along a coordinate (in this case layer), based on index. \n",
    "\n",
    "Python's indexing is 0-based, so to select the first layer by index, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thickness.isel(layer=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General indexing rules apply. To get the last one, use -1 as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thickness.isel(layer=-1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, unfortunately, our friendly geologist didn't get the memo that we'd like 14\n",
    "layers, rather than 7. We'll have to improvise an additional 7 layers!\n",
    "\n",
    "This is pretty easy: we simply select every layer twice, thereby creating a\n",
    "fourteen layered model. Of course, we'll have to halve the layer thickness to\n",
    "preserve total thickness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doubled_layers = np.repeat(thickness[\"layer\"], 2)\n",
    "new_thickness = 0.5 * thickness.sel(layer=doubled_layers)\n",
    "\n",
    "print(new_thickness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer coordinates aren't quite right yet, so we'll update those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_thickness[\"layer\"] = np.arange(1, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that have a fourteen layer thickness model, we can multiply the thicknesses\n",
    "by hydraulic conductivity to compute transmissitivities.\n",
    "\n",
    "Somebody has left them in ESRII ASCII format, but apart from slower read times,\n",
    "it's not an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kh = imod.rasterio.open(\"data/kh/kh_l*.asc\")\n",
    "kD = new_thickness * kh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the total transmissitivity, summed over all layers look like?\n",
    "\n",
    "Xarray provides methods to perform mathematical operations over one or more\n",
    "dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_kD = kD.sum(\"layer\")\n",
    "print(total_kD)\n",
    "total_kD.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.sum()`` is a reducing function: the output is `reduced` by one or more\n",
    "dimensions.  Similarly, ``.mean()``, ``.min()``, and ``.max()``, ``.count()``\n",
    "are also reducing functions.\n",
    "\n",
    "To compute the mean transmissivity per layer, we reduce over ``y`` and ``x``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_kD = kD.mean([\"y\", \"x\"])\n",
    "print(mean_kD.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more functions that operate over one or more dimensions, although\n",
    "they do not reduce.  These are for example ``.diff()`` or ``.cumsum()`` (for\n",
    "difference and cumulative sum).\n",
    "\n",
    "``.cumsum()`` can be used to here to compute new tops and bottoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bot = top.sel(layer=1) - new_thickness.cumsum(\"layer\")\n",
    "new_top = new_bot + new_thickness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``imod`` package provides some utilities for saving multi-dimensional data\n",
    "to geospatial formats. For IDFs, we have defined the ``imod.idf.save()``\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imod.idf.save(\"output/thickness\", new_thickness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``.save()`` function automatically creates the necessary IDF files to store\n",
    "all the layers. The file names are automatically generated by appending the\n",
    "additional dimensions. It also adds the ``.idf`` extension.\n",
    "\n",
    "The ``imod.idf.save()`` function differs from the ``imod.idf.write()`` function.\n",
    "The ``write()`` function only writes a `single` IDF file, which must therefore\n",
    "be two dimensional, with as dimensions only ``y`` and ``x``. ``write()`` also\n",
    "does not attempt to come up with a file name, but writes it to exactly the file\n",
    "you specify.\n",
    "\n",
    "Let's check the result of ``.save()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xarray, Dask, and lazy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a careful observer, you might've noticed we called ``.compute`` above when printing the results of the mean transmissivity per layer:\n",
    "\n",
    "```\n",
    "mean_kD = kD.mean([\"y\", \"x\"])\n",
    "print(mean_kD.compute())\n",
    "```\n",
    "\n",
    "This is what happens when we don't call ``.compute()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_kD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see our output here as an array of seven numbers as before:\n",
    "\n",
    "``array([ 14.43255,  40.48827, 135.03757, 130.95311, 229.52626, 190.12553, 216.42268], dtype=float32)``\n",
    "       \n",
    "Instead we get:\n",
    "\n",
    "``dask.array<mean_agg-aggregate, shape=(7,), dtype=float32, chunksize=(1,), chunktype=numpy.ndarray>``\n",
    "\n",
    "The reason is [lazy evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation). In lazy evaluation, operations (like subtractions and addition) are not executed immediately. Rather, the operation is added to a \"to do list\".\n",
    "The ``imod.idf.open`` operation is lazy in terms of loading data; the data of the IDFs isn't read into memory immediately.\n",
    "\n",
    "Let's check this \"to do list\" for ``top``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual IDF files are read (one read statement for every IDF), and then they are stacked to form the ``top`` DataArray.\n",
    "\n",
    "For ``thickness``, we expect to see:\n",
    "\n",
    "1. Creation of the ``top`` array.\n",
    "2. Creation of the ``bot`` array.\n",
    "3. Subtraction of the two to form ``thickness``.\n",
    "\n",
    "These steps are clearly visible in the following graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thickness.data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask arrays are like numpy arrays, except that result are not computed **eagerly** and a single array may consists of multiple **chunks**. This has two primary benefits: \n",
    "\n",
    "1. Calculations can be efficiently parallelized (e.g. one chunk per processor).\n",
    "2. Larger than memory datasets can be handled, as long as the **chunks** fit within memory.\n",
    "\n",
    "To give a specific example: we could open many more IDFs than fit within memory, and still compute mean values with a single operation!\n",
    "\n",
    "In Jupyter notebooks, there's a fancy HTML representation of these lazy, chunked arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thickness.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To force the computation, we call ``.compute()``. Essentially, ``.compute()`` turns a dask array into an ordinary numpy array. Generally, it isn't necessary to call ``.compute()`` yourself. For example, during plotting the result will be computed as well, or when writing to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(thickness.data))\n",
    "print(type(thickness.compute().data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've only dealt with spatial dimensions: ``layer``, ``y``, and ``x``.\n",
    "We're going to look at time varying data next. \n",
    "\n",
    "As mentioned above, the ``imod.idf.open()`` function automatically parses\n",
    "timestamps, and we can open a simulation output as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = imod.idf.open(\"data/head/head_*_l1.idf\")\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timestamps in the file names are automatically converted to a ``datetime``\n",
    "\n",
    "If we select a single point in space and plot it, we automatically get a\n",
    "timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = head.sel(x=450_000, y=86_000, method=\"nearest\")\n",
    "timeseries.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the simulated head with measurements.\n",
    "\n",
    "Let's load some CSV data of a piezometer, of the location selected above,\n",
    "and plot the measured and simulated head together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"data/piezometer.csv\", index_col=[\"date\"], parse_dates=[\"date\"])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "timeseries.plot(ax=ax)\n",
    "df[\"measured\"].plot(ax=ax, style=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, have a look at how the results hold up spatially.\n",
    "\n",
    "Using geopandas we can load a shapefile that contains the mean measurement value for a set of piezometers.\n",
    "We can compute the mean simulated head by calling ``.mean(\"time\")``.\n",
    "\n",
    "We finish by plotting the measurements\n",
    "on top of the mean simulation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "gdf = gpd.read_file(\"data/piezometers.shp\")\n",
    "\n",
    "mean_head = head.mean(\"time\").squeeze(\"layer\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mean_head.plot.contourf(levels=np.arange(-5.0, 5.0), ax=ax)\n",
    "gdf.plot(ax=ax, column=\"measured\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily make a scatter plot by selecting the values from ``mean_head`` based on x and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[\"simulated\"] = imod.select.points_values(mean_head, x=gdf.geometry.x.values, y=gdf.geometry.y.values)\n",
    "df = gdf[[\"simulated\", \"measured\"]]\n",
    "df.plot.scatter(\"measured\", \"simulated\")\n",
    "plt.plot([-6.0, -1.0], [-6.0, -1.0], linestyle='--', color=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the absolute difference, and plot it spatially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[\"abs_diff\"] = (gdf[\"simulated\"] - gdf[\"measured\"]).abs()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mean_head.plot.contourf(levels=np.arange(-5.0, 5.0), ax=ax)\n",
    "gdf.plot(ax=ax, column=\"abs_diff\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``imod`` package also supports writing tabular data to IPF files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[\"x\"] = gdf.geometry.x\n",
    "gdf[\"y\"] = gdf.geometry.y\n",
    "to_ipf = gdf[[\"x\", \"y\", \"id\", \"measured\", \"abs_diff\"]]\n",
    "imod.ipf.save(\"output/piezometers.ipf\", to_ipf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have:\n",
    "\n",
    "* Loaded many IDFs into Python using ``imod`` functions.   \n",
    "* Selected data by value and index.\n",
    "* Used selection to create a new DataArray.   \n",
    "* Applied mathematical operation over specific dimensions.\n",
    "* Written data back to disk.\n",
    "* Loaded time-dependent simulation data into Python.\n",
    "* Compared it with (tabular) measurements.\n",
    "* Written the resulting table to an IPF file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
